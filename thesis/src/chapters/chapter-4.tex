\chapter{\project\ Internals}
\label{chapter:Chapter 4}

As the name specifies, this chapter will cover architectural and implementation details.
First, a detailed overview of the architecture is presented, which describes what are the main modules and how they interact with each other and some other details regarding configuration files introduced in earlier chapters.
Next, the application internals will be presented in detail in \labelindexref{Section}{sec:implementation}.

\section{Architecture Overview}
\label{sec:arh-overview}

This section is going to be introduced by \labelindexref{Figure}{img:class-overview-simplified}, which shows a Class Diagram that describes the overview of the application architecture.
It shows a simplified version of the full class-diagram for a better understanding of the structure while not losing the important aspects.

\fig[scale=0.65]{src/img/diagrams/class-overview-simplified.pdf}{img:class-overview-simplified}{Class diagram overview simplified}

\subsection{From Config File to Execution}
\label{sub-sec:arh-config-to-exec}

The entry point to this application are the \textbf{config files} which in general terms consist of one or more persistent information about some state of a given application.
In \labelindexref{Section}{sec:hosts-and-topologies} it can be clearly seen that this part is encapsulated inside the \texttt{VmConfig} class, which holds the informations from each config file.
These configurations are vital for the ability to easily specify different kinds of network topologies and types of Hosts.

When \project\ is run, the first thing that is created is the \texttt{CLI} class which then instantiates \texttt{Vlab}, an object that holds every necessary information for this application.
Then, all the config files are read from disk, parsed and then all the \texttt{Node} objects  are created according to these files. The \texttt{Nodes} can be either instances of \texttt{Switch} or \texttt{Host} classes.
From this point, it is up to the user what he does next, as he can choose to start the VMs, and then run some commands on them, or simply exit the application.
The user commands are parsed with help from the CLI.

\subsection{A More Detailed View of Configs}
\label{sub-sec:configs-detalied}

\subsubsection{Topology Configs}
\label{sub-sub-sec:topology-configs}

\labelindexref{Section}{sub-sec:generic-network-topologies} introduced the configuration for network topologies and how these can be generated with an external tool, but it did not specify their exact format nor other details.
The file type for every config of \project\ is JSON, which is a very simple format derived from how objects are represented in JavaScript\footnote{\url{http://msdn.microsoft.com/en-us/library/bb299886.aspx}}.
One such file is given as an example for a simple topology in \labelindexref{Listing}{lst:config-one-switch-two-hosts}.
Drilling down a little deeper within this file, there can be found three main objects, which are actually arrays that contain other objects:

\begin{itemize}
  \item \texttt{Host} object details related to test interface
  \item \texttt{Switch} object details
  \item Links between \texttt{Nodes} and \texttt{Switches}
\end{itemize}

\texttt{Host} objects from this config file contain several options, but the only ones that are really used by \project\ are the hostname and the ip, which serve as a means of specifying the IP address which will be used on the testing interface, as well as how the \texttt{Host} will be named.
Besides these two properties, the number of Virtual Machines that need to be created is inferred from the number of \texttt{Hosts}, avoiding the usage of more resources than needed for the purpose of the given test case.
Other informations more specific to \texttt{Hosts} that are topology independent are described in another config file, detailed in the next subsection.

For \texttt{Switches} there is no need to use anything else other than the hostname option, which has the same purpose as the one for \texttt{Hosts}.
In future versions of \project\ there could be other options to be specified, but for this implementation is limited to the use of a hostname.

The links between the \texttt{Nodes} are defined as a list of adjacency in the config file, specifying for each connection the source and destination.
Very intuitively, the names of the fields are \texttt{src} and \texttt{dest}, the values having to be specified by the hostnames of the corresponding \texttt{Nodes} that are linked together.

Any other field defined in the topology config file is not related to \project\ and was initially designed for MiniEdit, which saves more informations about the position on the screen of the nodes, thus are not needed here.

\subsubsection{Host Specific Configs}
\label{sub-sub-sec:vm-configs}

Besides specifying a topology, one must also include a few other configurations specific to \project\ implementation, like where to load the kernel image from, how much RAM to use for \texttt{Hosts}, etc.
This VM config file is made from a couple main properties that are simple key-value pairs, like \texttt{qemu_binary} which specifies which version of Qemu to be run, \texttt{max_ram} which represents the maximum amount of RAM available for each VM and \texttt{base_name} which is used for generating host specific file names.

One of the most important parts of this config is the \texttt{kernel_image} object, which has a few properties, like \texttt{dir} and \texttt{image_name} which together specify how is the Linux Kernel image named and where to find it on disk.
Also, there are a couple parameters specific to kernel \texttt{init}, like what script to run at init and where to load it from, where to mount the root and in what mode, some of which being specific to how Qemu system creates shared folders between the physical machine.
The mechanism of folder sharing will be described in more detail in \labelindexref{Section}{sec:implementation} which treats other particularities.

Further on, the file needs to contain some other properties within an array named \texttt{properties}, even though not all of them are required.
These objects are defined to be extensible, such that when new kinds of properties need to be added, little to no changes to be made in the code of the application.
The fields that are mandatory for every property are \texttt{dev}, \texttt{type} and \texttt{id} which define the device and the logical type used in Qemu, and a unique identifier for each property.
Based on the device and type, other fields must be defined, like a \texttt{role} and a \texttt{socket_management} when the \texttt{dev} is set to the \texttt{chardev} value.
For \texttt{fsdev} devices, there needs to be set the \texttt{path} \texttt{mount_tag}, \texttt{device_id} and \texttt{device_type} fields to corresponding values.
The last implemented type of device is \texttt{netdev} which defines a virtualized networking device.
All these types can be seen more clearly in the sample configuration file in \labelindexref{Listing}{lst:vm-config}.

\subsection{Classes Description and Interaction}
\label{sub-sec:classes-description-interaction}

At the beginning of this chapter we introduced a simplified Class Diagram for \project\ in \labelindexref{Figure}{img:class-overview-simplified}.
This section aims to explain the connections of these classes and their roles within the entire system.
We barely touched the surface of the application by presenting configs and what they need to contain.

\subsubsection{CLI and Vlab Classes}
\label{sub-sub-sec:cli-vlab-classes}

The core class in this diagram is \texttt{Vlab}, which is responsible to coordinate the logic of the structure and to create all the necessary objects in the right order while running.
It receives commands from the user with help from the \texttt{CLI} class, which acts as an adapter, intercepting every request from the user and passing them to the other objects that actually implement the command themselves.
In fact, the \texttt{CLI} implementation inherits some behavior from the \texttt{Cmd} class in the Python Standard Library\footnote{\url{https://docs.python.org/2/library/cmd.html}} which is a helper class intended to be a starting point for line-oriented command interpreters.

\texttt{Vlab} basically reads the config files when it is created with help from \texttt{VmConfigLoader} class, and then creates exactly the number \texttt{Hosts} and \texttt{Switches} defined in the topology.
When it receives the \texttt{start} or \texttt{stop} commands from \texttt{CLI}, it then simply iterates over the \texttt{Nodes} and calls the corresponding generic method on those objects.
It was designed such that the config files can be reloaded without actually restarting the entire application.
Though, the nodes must be stopped before reloading the configs, otherwise the user may end up with some nodes that cannot be stopped from within \project\ anymore.

Next, the logic behind topology nodes is encapsulated within the \texttt{Node} interface, which defines the basic operations that each entity should be able to execute.
Some nodes may have some additional operations associated with them, thus they will implement the \texttt{Node} class and add their own capabilities as necessary.
This pattern is used in order to make the application extensible and easy to maintain when new types of nodes must be supported.

\subsubsection{Host Class}
\label{sub-sub-sec:host-class}

One of the \texttt{Node} implementations is the \texttt{Host} class.
It encapsulates the behavior of a virtualized host, which in Mininet was a simple process and in \project\ is a fully capable Virtual Machine that runs a Linux Kenrnel.
It's responsibility is to take care of the representation of an end-user machine which knows how to start and stop itself and how to receive commands and execute them reliably.
Although the interface is pretty simple, the inner representation is a little more complex and here is where the \texttt{VmHandler} class comes handful by handling all the communication between \project\ and the corresponding Qemu VM.
Before starting the VM, the \texttt{VmHandler} requests the command line that needs to be run at startup from its \texttt{VmConfig} instance and only executes that command as a new process as if the user himself typed that command in a terminal.

After starting it, the \texttt{Host} is completely detached from the VM with which it can only communicate via special messages sent either through a serial interface or through a SSH connection.
More details on this topic will be covered in \labelindexref{Section}{sec:implementation}.

Also, the networking interfaces need to be configured as well and this is accomplished after the Linux Kernel has been initialized only, such that the VM is running and can receive messages from the object that created it.
How is this synchronization implemented and other details are covered as well in \labelindexref{Section}{sec:implementation}.

\subsubsection{Switch Class}
\label{sub-sub-sec:switch-class}

The other implementation of \texttt{Node} is the class named \texttt{Switch}, which emulates the physical device that interconnects two or more other nodes.
It is implemented rather simple, as it uses the \texttt{brctl}\footnote{\url{http://linuxcommand.org/man_pages/brctl8.html}} command, which simply configures a bridge.
By using this command for simulating a switch, \project\ simplifies its architecture and resource consumption, as there is no need to spawn yet another VM just for switching.
This is of course possible because the Linux Kernel has this feature natively implemented and it doesn't require other settings to be done.

After the \texttt{Switch} is started, it can have attached to it several end-point \texttt{Hosts} which in the end will have simulated physical connectivity from one to another, the same way a physical switch connects several machines with Ethernet cables.\footnote{\url{http://www.tldp.org/HOWTO/BRIDGE-STP-HOWTO/set-up-the-bridge.html}}


\section{Further Implementation Details}
\label{sec:implementation}

Until now we covered the most important components of \project\ and its architecture.
This section will focus more on other implementation details which have not been covered yet.
First, we will discuss some aspects related to interfaces and communication with the VMs and then we will give some details regarding the startup and initialization process.
The presentation of these particularities is intentionally made in reverse chronological in relation to the states of a VM.
This is because it is easier to understand the internals if we begin with what the user actually needs to know and continue with the other aspects that are not as important for him.

\subsection{Management and Testing Interfaces}
\label{sub-sec:mgmt-and-testing-intf}

One of the main things that the user needs to use are the network interfaces created for testing.
They are the core of this project, as the network modules cannot be tested without the existence of interfaces.
Every network interface is created on the physical host machine as a \texttt{Tap} device with the \texttt{tunctl}\footnote{\url{http://linux.die.net/man/8/tunctl}} command.
A broad view of how the entire networking system works in \project\ is presented in \labelindexref{Figure}{img:tap-devices}.

\fig[scale=1.0]{src/img/diagrams/tap-devices.pdf}{img:tap-devices}{Representation of Tap Network Devices}

Following is presented how this system works.
The physical machine has an interface \texttt{eth0} which communicates with the rest of the world and has several switches attached to it.
They are created with the already mentioned \texttt{brctl} command.
Next, for each interface that needs to be created on the Virtual Machines, there must be added a \texttt{tap} interface with the \texttt{tunctl} commands.
The \texttt{taps} have one end associated with one of the switches and the other end represents the entry point to the VMs.
To make it possible for the VM to communicate with host machine, the \texttt{tap} and the associated interface on the guest must have IP addresses set within the same network range.\abbrev{IP}{Internet Protocol}

\labelindexref{Figure}{img:tap-devices} incorporates only a couple of the entire interfaces that are created for each VM for more clarity.
Fundamentally, any interface that is added on a guest VM must be paired with a \texttt{tap} interface on the host as described above, though it is not mandatory to use a bridge between the \texttt{taps} and the host's \texttt{eth0} interface.
The bridges are only used when testing topologies that use switches, thus usually there are some interfaces that will have their \texttt{tap} directly connected to the host.

Besides the testing interfaces, \project\ supplies one management interface for each VM.
This is in order to give further flexibility to the user if he wants to connect to one VM for debugging via SSH.
Through this interface are sent all the commands that a user issues in \project\ to a specific \texttt{Host}.
Also, redundantly, it is provided a serial interface for the same purpose.
These management interfaces will be treated separately in \labelref{Next Chapter}{sub-sec:communication-with-vm} where we discuss more about the communication and administration of the \texttt{Host} VMs.

\subsection{Management of VMs}
\label{sub-sec:communication-with-vm}

As we already covered in previous sections, a \texttt{Host} is in fact a VM that is run over Qemu.
When a \texttt{Host} is started, a new Qemu VM is created with help from the \texttt{VmHandler} class which runs a pretty long command for starting up a new VM.
Through the parameters of this command one can specify several kinds of interfaces that can be added to the VM and how to interact with them.
One of these interfaces is the Qemu Monitor presented next.

\subsubsection{Qemu Monitor}
\label{sub-sub-sec:qemu-monitor}

This is the basic interface provided by Qemu for administrating the type of hardware emulated or virtualized by the VM it is linked to\footnote{\url{http://en.wikibooks.org/wiki/QEMU/Monitor}}.
Through this interface one can change devices and many properties of the VM.
More importantly, he can add or remove network interfaces, which are the focus of \project.

By default, Qemu opens a separate window where the Monitor can be accessed.
Because \project\ aims to automate a lot of work for the user, this window is no longer spawned when booting the VM and instead a new named socket file is created on disk.
The user can then choose if and when to connect to this interface in order to manually modify the settings of the current Virtual Machine.
There are many possibilities to connect to this Qemu Monitor \cite{qemu-monitor}, and \project\ chose to use the \texttt{socat} command\footnote{\url{http://manpages.ubuntu.com/manpages/trusty/man1/filan.1.html}} which is the simpler possibility from the many existing.
Also, another reason for using this mode for connecting to the Qemu Monitor is that it is easier to send the commands programmatically for adding or removing network interfaces at runtime.
This is actually exactly what \project\ does when a VM is started and is connected to something else in a topology: it sends commands to the Qemu Monitor to create the simulated physical network interface, which is then configured after booting is complete.
The task of sending commands is accomplished with help from the \texttt{socket} Python module\footnote{\url{https://docs.python.org/2/library/socket.html}} which permits easy interaction with sockets from within the code.

Even though this interface is intended for internal use only, one can still choose to manually connect to it and add or remove network interfaces or other devices on the VM.
It is just a matter of entering a command like the following:

\lstset{label=lst:socat-example}
\begin{lstlisting}
socat - /tmp/VM-test1/vm-monitor-console.socket
\end{lstlisting}

This command makes the user to connect directly to the serial interface to which the Qemu Monitor is connected, and then he can simply issue the desired requests.

\subsubsection{Administration via SSH}
\label{sub-sub-sec:admin-SSH}

In \labelindexref{Section}{sub-sec:mgmt-and-testing-intf} we talked about how the interfaces are linked in \project.
We mentioned that a management Ethernet interface is used.
It has a fixed IP and a SSH server is running inside the VM such that remote connections can be made through this interface.
Most of the commands that a user can enter in \project\ CLI are sent through a SSH connection, making it truly flexible.
Also, as described in \labelindexref{Section}{sub-sec:spawn-xterm}, \project\ can create Xterm connections to each VM.
These are realized as well by spawning SSH connections to those VMs.

Because we are running a minimal Linux Kernel, we don't actually want a full-featured SSH server, so the first option which we actually chose was to use Dropbear\footnote{\url{https://matt.ucc.asn.au/dropbear/dropbear.html}}, a lightweight server that can be built statically and still uses less than 5MB of disk space.
The resulting binary is copied to the shared folder of the VMs, meaning that the disk footprint consists just of one binary that is used by every \texttt{Host}, so it is almost unnoticed.

Another reason for the adding the interface for administration vis SSH was the fact that through other means, one could only issue commands without wanting to check back the results.
With SSH, he can now use more fancy programs remotely, like starting a text editor like Vim\footnote{\url{http://www.vim.org/}} or checking system status with \texttt{htop}\footnote{\url{http://hisham.hm/htop/}}.
Of course, \project\ doesn't need text editors and other programs to be run, but needs a reliable channel for communication with the \texttt{Hosts} and SSH represents one of the best candidates for this purpose.

For being able to actually use it, one must either connect to the remote host with a known password, or via the public-key authentication mechanism\footnote{\url{https://help.ubuntu.com/community/SSH/OpenSSH/Keys}}.
Using a password would simply break any means of automation, so the actual option was to use public and private key pairs.
The public key would be appended in the \texttt{authorized_keys} file of each \texttt{Host}, which file is actually loaded directly in memory for avoiding conflicts, and the private key would stay in a separate file in the shared folder or anywhere else.
This mechanism ensures that the SSH connections will work without being prompted for a password.
Even though the user is not asked for a password anymore, this doesn't solve the problem where he is required to add the \texttt{Host} to the \texttt{known_hosts} file the first time he connects to that \texttt{Host}.
While the user can manually enter ``yes'' as an answer for that prompt, it still breaks scripts and automated thinks that need SSH.
Though this seemed like a dead end, we found \texttt{paramiko}\footnote{\url{http://www.paramiko.org/}}, a Python module that implements the SSHv2 protocol.
It not only provides an easy to use interface for interaction with SSH connections, but it also supports a mode in which it does not require the \texttt{known_hosts} file.
This basically solved all the problems related to interactive prompts that the SSH protocol uses.

Further on, after having this issue solved, we found another problem.
When connecting to a remote shell through SSH, the \texttt{PATH} environment variable is not the same with the one that is created for a normal shell.
This is because the SSH command execution shell is a non-interactive shell and does not use the \texttt{\textasciitilde/.bashrc} or \texttt{\textasciitilde/.profile} files, but searches through the \texttt{BASH_ENV} variable for the file to be read and executed, but \texttt{PATH} is not used to search for the file name\footnote{\url{http://stackoverflow.com/questions/216202}}.
The obvious solution for this is to prepend every command that is sent from \project\ to \texttt{Hosts} like in the following example where the command needed to be executed is a simple \texttt{ls}, for better understanding:

\lstset{label=lst:ssh-prepend-example}
\begin{lstlisting}
source ~/.bashrc; ls -al
\end{lstlisting}

The \texttt{\textasciitilde/.bashrc} file could simply contain a line that exports the \texttt{PATH} variable like in this example:

\lstset{label=lst:ssh-bashrc-example}
\begin{lstlisting}
export PATH=$HOME/bin:/bin:/usr/local/bin:/usr/bin:/sbin:/usr/local/sbin:/usr/sbin
\end{lstlisting}

That being said, all the inconveniences that made the automation of sending commands over SSH have been surpassed.
To ensure redundancy though, there was introduced a serial interface as well, for the same purpose as the \texttt{ethernet} interface, presented next.

\subsubsection{Administration via Serial Interface}
\label{sub-sub-sec:admin-serial}

This serial interface plays an important role in using \project.
For normal cases it wouldn't be needed, but it serves as a means of backup way of connecting to the VM when everything else fails, i.e. the networking stack is not working anymore because of some bad driver, or other causes.
Basically it is just created when the VM is started, and then it can be accessed through a socket file located on disk.
Similarly to the serial interface linked to Qemu Monitor described in the section about \labelref{Qemu Monitor}{sub-sub-sec:qemu-monitor}, this can be accessed with a command like the following:

\lstset{label=lst:socat-serial-example}
\begin{lstlisting}
socat - /tmp/VM-test1/vm-mgmt-console.socket
\end{lstlisting}

The user then can then manually enter commands to debug what happened on the VM and see if he can fix the issues.
It is also useful for using GDB remotely, for the same purpose of debugging.

\subsection{VM Init Script}
\label{sub-sec:vm-init-script}

In this section we will discuss about what happens when a VM boots up, and how \project\ is announced when a \texttt{Host} has finished the booting process.

To begin, the Linux Kernel needs an \texttt{init} process that is the parent for every other process in the system.
This process can be anything from a small shell script that continuously spawns a shell if there is an attempt to close the newly created shell, to the more complicated way of running multiple initialization scripts.
Because we do not actually need everything that is normally initialized in a full Linux distribution, we chose to implement our own \texttt{script}, that does the basic task of spawning a shell and some more initialization tasks needed for this application.
This is mostly responsible for setting up the hostname, mounting the root, the shared home and other directories and for setting up the interfaces.
It also configures an IP on the management \texttt{ethernet} interface and then starts the dropbear SSH server.
The last thing it does before spawning a shell is to announce the physical host machine that this VM has finished the booting process and can begin to execute any other user requests.

To sum up, this script is pretty simple and lightweight such that only the necessary things are created at startup, with no clutter.
There are no daemons, nor other magic scripts involved, so its structure is easy to understand and to maintain if changes are needed in the future.
